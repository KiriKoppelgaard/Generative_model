{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data (lesson 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting of by downloading data and looking at the data, which we need to clean. We observe following\n",
    "- Illustration texts\n",
    "- The text file contain multiple stories, these should be split into multiple files in order for the model to know there is groupings (Extension: will be reviewed in lesson 5)\n",
    "- The file contains a number of punctuations (,;.* etc.), which are irrelevant for the model\n",
    "- Capital letters as headers\n",
    "\n",
    "The input sequences are at first at 50 across sentences, chapters and stories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to load documents into memory\n",
    "def load_doc(filename):\n",
    "\tfile = open(filename, 'r', encoding=\"utf8\") #loading an existing file\n",
    "\ttext = file.read() #opening the file and assigning it to the variable text\n",
    "\tfile.close() #close the file\n",
    "\treturn text #output = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\au591024\\\\Desktop\\\\Data\")\n",
    "path = \"C:\\\\Users\\\\au591024\\\\Desktop\\\\Data\"\n",
    "\n",
    "#concatenate text files\n",
    "filenames = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "with open('adventures.txt', 'w', encoding=\"utf8\") as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, encoding=\"utf8\") as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load document\n",
    "in_filename = 'adventures.txt'#specifying the filename of the data we wish to load\n",
    "doc = load_doc(in_filename) #loading the file\n",
    "print(doc[:10000]) #printing the first 200 characters of the loading document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data (lesson 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lesson 1, we observed several potential issues in the data, which we need to remove from the data. This is done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#removing illustration descriptions\n",
    "doc = re.sub(r'\\[[^)]*\\]', '', doc) #using re.sub function and regular expressions \n",
    "#[ - an opening bracket \n",
    "#[^()]* - zero or more characters other than those defined, that is, any characters other than [ and ]\n",
    "#\\] - a closing bracket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing headers\n",
    "doc = re.sub(r'[A-Z]{2,}','', doc) #replacing capital letters longer than 1 with nothing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing special characters, since one of the deliminators\n",
    "#doc = doc.replace(\"'\", '')\n",
    "\n",
    "#deliminators \n",
    "#deliminators = c('THE FIR TREE', 'LITTLE TUK', 'THE UGLY DUCKLING', 'LITTLE IDA'S FLOWERS', 'THE STEADFAST TIN SOLDIER, LITTLE THUMBELINA, SUNSHINE STORIES', 'THE DARNING-NEEDLE', 'THE LITTLE MATCH GIRL', 'THE LOVING PAIR', 'THE LEAPING MATCH', 'THE HAPPY FAMILY, THE GREENIES, 'OLE-LUK-OIE', 'THE DREAM GOD', 'THE MONEY BOX', 'ELDER-TREE MOTHER', 'THE SNOW QUEEN', 'THE ROSES AND THE SPARROWS', 'THE OLD HOUSE', 'THE CONCEITED APPLE BRANCH' \n",
    "\n",
    "#splitting the file into strings consisting of one adventure\n",
    "#doc = doc.split ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc): #making a function\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token - https://www.geeksforgeeks.org/python-maketrans-translate-functions/\n",
    "\ttable = str.maketrans('', '', string.punctuation) #Third argument specifies the wished deleted items\n",
    "\ttokens = [w.translate(table) for w in tokens] #translate applies the translation table applied on the looping through the tokens list\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()] #Replace word with word, if the word is alphabetic\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = clean_doc(doc) #using the clean function to cleaning our document\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens)) #printing the number of tokens\n",
    "print('Unique Tokens: %d' % len(set(tokens))) #printing the number of unique tokens by grouping similair tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1 #defining the length of the sequence\n",
    "sequences = list() #creating an empty list\n",
    "for i in range(length, len(tokens)): #range takes two arguments: start point and end point\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq) #-join returns a string in which the string elements of sequence have been joined by str separator\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename): #creating a saving function\n",
    "\tdata = '\\n'.join(lines) #joining the lines seperated by 'enter'\n",
    "\tfile = open(filename, 'w', encoding=\"utf8\") #create empty new file\n",
    "\tfile.write(data) #input the data in the empty file\n",
    "\tfile.close() #close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'fairytales_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model (Lesson 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the cleaned data\n",
    "in_filename = 'fairytales_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "\n",
    "#split the text by lineshifts creating a list of 51 items long string\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer() #assign the tokenizer function to a variable\n",
    "tokenizer.fit_on_texts(lines) #function of keras finds all of the unique words in the data and assigns each a unique integer\n",
    "sequences = tokenizer.texts_to_sequences(lines) #translating the input lines into integers\n",
    "tokenizer.word_index #checking the dictionary of the transformed wordsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating vocabulary size to estimate the size of the embedding layer\n",
    "vocab_size = len(tokenizer.word_index) + 1 #since indexing of array are zero-offset, the index of the vocabulary must be one larger than the length\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences) #transforming the sequens of integers to arrays\n",
    "X, y = sequences[:,:-1], sequences[:,-1] #defining X (input sequences) and y (output words)\n",
    "y = to_categorical(y, num_classes=vocab_size) #to_categorical converts a class vector (integers) to binary class matrix\n",
    "seq_length = X.shape[1] #gives you the dimension of the array, which we put to be 50\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential() #assigning the sequential function to a model\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length)) #defining embedding layer size\n",
    "model.add(LSTM(100, return_sequences=True)) #adding layer of nodes\n",
    "model.add(LSTM(100))  #adding layer of nodes\n",
    "model.add(Dense(100, activation='relu')) #specifying the structure of the hidden layer, recu is an argument of a rectified linear unit. \n",
    "model.add(Dense(vocab_size, activation='softmax')) #using the softmax function to creating probabilities\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #the compile function configures the model for training specifying the categorical cross entropy loss\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100) #training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "doc = load_doc('fairytales_sequences.txt')\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Specifying input sequences length to prompt the model\n",
    "seq_length = len(lines[3].split()) - 1 #splitting the sequences into words, counting them -1\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "model= load_model('model.h5')\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb')) #r for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting a seed text\n",
    "seed_text = lines[randint(0,len(lines))] #returns random integer between 0 and how many lines there is, and indexes this. \n",
    "print(seed_text + '\\n') #prints the selected text\n",
    "len(seed_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating the input text to integers\n",
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can predict the next word directly by calling model.predict_classes() that will return the index of the word with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for each word\n",
    "yhat = model.predict_classes(encoded, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking up the index in the Tokenizers mapping to get the associated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a loop to translate integer to word\n",
    "out_word = ''\n",
    "for word, index in tokenizer.word_index.items():\n",
    "\tif index == yhat:\n",
    "\t\tout_word = word\n",
    "\t\tbreak\n",
    "\n",
    "out_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sequences will get too long, in order to keep them to 50 items using the following function, which pads sequences to the same lengt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the max length to be 50 items by removing items from the beginnning of the sequence\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function, which generates the predicted output\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list() #make an empty list\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 100)\n",
    "print(generated)\n",
    "\n",
    "#saving generated text\n",
    "out_filename = 'first_output.txt'\n",
    "save_doc(generated, out_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to be consider to advance the model: \n",
    "\n",
    "- We could process the data so that the model only ever deals with self-contained sentences and pad or truncate the text to meet this requirement for each input sequence. You could explore this as an extension to this tutorial.\n",
    "- MORE DATA!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
