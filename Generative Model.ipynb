{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genrative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start of by loading the required packages needed in the coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Exploring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have downloaded fairy tales from the webpage Gutenberg.org. After downloading data and looking at the data, which we need to clean. We observe following\n",
    "- Illustration texts\n",
    "- The text file contain multiple stories, these should be split into multiple files in order for the model to know there is groupings (Extension: will be reviewed in lesson 5)\n",
    "- The file contains a number of punctuations (,;.* etc.), which are irrelevant for the model\n",
    "- Capital letters as headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a function, which allows us to load the data into the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to load documents into memory\n",
    "def load_doc(filename):\n",
    "\tfile = open(filename, 'r', encoding=\"utf8\") #loading an existing file\n",
    "\ttext = file.read() #opening the file and assigning it to the variable text\n",
    "\tfile.close() #close the file\n",
    "\treturn text #output = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use multiple fairy tales and concatenate them into a single file to can load them easily into the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate text files\n",
    "path = 'Data' #Setting the path\n",
    " \n",
    "files = os.listdir(path) #assign the filenames in the directory to the variable files\n",
    "\n",
    "#with open('Data/adventures.txt', 'w', encoding=\"utf8\") as outfile: #writing a loop adds each fairy tale to a signle file\n",
    "#    for fname in files:\n",
    "#        with open(fname, encoding=\"utf8\") as infile:\n",
    "#            outfile.write(infile.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the loading function to the created document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿THE SNOW QUEEN\n",
      "\n",
      "A TALE IN SEVEN STORIES\n",
      "\n",
      "FIRST STORY\n",
      "\n",
      "WHICH DEALS WITH A MIRROR AND ITS FRAGMENTS\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'Data/adventures.txt'#specifying the filename of the data we wish to load\n",
    "doc = load_doc(in_filename) #loading the file\n",
    "print(doc[:100]) #printing the first 200 characters of the loading document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we have created a single document containing the fairy tales, we need to use for the model and created a function for loading it into the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data (lesson 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lesson 1, we observed several potential issues in the data, which we need to remove from the data. This is done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#removing illustration descriptions\n",
    "doc = re.sub(r'\\[[^)]*\\]', '', doc) #using re.sub function and regular expressions \n",
    "#[ - an opening bracket \n",
    "#[^()]* - zero or more characters other than those defined, that is, any characters other than [ and ]\n",
    "#\\] - a closing bracket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing headers\n",
    "doc = re.sub(r'[A-Z]{2,}','', doc) #replacing capital letters longer than 1 with nothing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc): #making a function\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token - https://www.geeksforgeeks.org/python-maketrans-translate-functions/\n",
    "\ttable = str.maketrans('', '', string.punctuation) #Third argument specifies the wished deleted items\n",
    "\ttokens = [w.translate(table) for w in tokens] #translate applies the translation table applied on the looping through the tokens list\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()] #Replace word with word, if the word is alphabetic\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'a', 'at', 'home', 'in', 'the', 'princes', 'palace', 'when', 'at', 'night', 'the', 'others', 'were', 'asleep', 'she', 'used', 'to', 'go', 'out', 'on', 'to', 'the', 'marble', 'steps', 'it', 'cooled', 'her', 'burning', 'feet', 'to', 'stand', 'in', 'the', 'cold', 'seawater', 'and', 'at', 'such', 'times', 'she', 'used', 'to', 'think', 'of', 'those', 'she', 'had', 'left', 'in', 'the', 'deep', 'one', 'night', 'her', 'sisters', 'came', 'arm', 'in', 'arm', 'they', 'sang', 'so', 'sorrowfully', 'as', 'they', 'swam', 'on', 'the', 'water', 'that', 'she', 'beckoned', 'to', 'them', 'and', 'they', 'recognised', 'her', 'and', 'told', 'her', 'how', 'she', 'had', 'grieved', 'them', 'all', 'after', 'that', 'they', 'visited', 'her', 'every', 'night', 'and', 'one', 'night', 'she', 'saw', 'a', 'long', 'way', 'out', 'her', 'old', 'grandmother', 'who', 'for', 'many', 'years', 'had', 'not', 'been', 'above', 'the', 'water', 'and', 'the', 'merman', 'king', 'with', 'his', 'crown', 'on', 'his', 'head', 'they', 'stretched', 'out', 'their', 'hands', 'towards', 'her', 'but', 'did', 'not', 'venture', 'so', 'close', 'to', 'land', 'as', 'her', 'sisters', 'day', 'by', 'day', 'she', 'became', 'dearer', 'to', 'the', 'prince', 'he', 'loved', 'her', 'as', 'one', 'loves', 'a', 'good', 'sweet', 'child', 'but', 'it', 'never', 'entered', 'his', 'head', 'to', 'make', 'her', 'his', 'queen', 'yet', 'unless', 'she', 'became', 'his', 'wife', 'she', 'would', 'never', 'win', 'an', 'everlasting', 'soul', 'but', 'on', 'his', 'wedding', 'morning', 'would', 'turn', 'to', 'seafoam', 'am', 'i', 'not']\n",
      "Total Tokens: 15574\n",
      "Unique Tokens: 2460\n"
     ]
    }
   ],
   "source": [
    "tokens = clean_doc(doc) #using the clean function to cleaning our document\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens)) #printing the number of tokens\n",
    "print('Unique Tokens: %d' % len(set(tokens))) #printing the number of unique tokens by grouping similair tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now left with only tokens from the documents, since we have cleaned the document from headers and punctuation and split them into single words. Next we need to arrange the tokens into sequences of 50 input items and 1 output item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 15523\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1 #defining the length of the sequence\n",
    "sequences = list() #creating an empty list\n",
    "for i in range(length, len(tokens)): #range takes two arguments: start point and end point\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq) #-join returns a string in which the string elements of sequence have been joined by str separator\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we save the sequences into a file, which can be loaded for the next lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename): #creating a saving function\n",
    "\tdata = '\\n'.join(lines) #joining the lines seperated by 'enter'\n",
    "\tfile = open(filename, 'w', encoding=\"utf8\") #create empty new file\n",
    "\tfile.write(data) #input the data in the empty file\n",
    "\tfile.close() #close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'Data/fairytales_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model (Lesson 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson we obtained a document consisting of clean sequences, which is ready for use in training our model. We start by loading these sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the cleaned data\n",
    "in_filename = 'Data/fairytales_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "\n",
    "#split the text by lineshifts creating a list of 51 items long string\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transform the sequences into to integers to allow us to create wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer() #assign the tokenizer function to a variable\n",
    "tokenizer.fit_on_texts(lines) #function of keras finds all of the unique words in the data and assigns each a unique integer\n",
    "sequences = tokenizer.texts_to_sequences(lines) #translating the input lines into integers\n",
    "#tokenizer.word_index #checking the dictionary of the transformed wordsb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the size of the embedding layer in the ANN, we calculate the vocabulary size. Next we seperate the sequences into input and output, so the model will based on the 50 elements long input sequence, will be able to predict the 51th word, which is the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2460"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating vocabulary size to estimate the size of the embedding layer\n",
    "vocab_size = len(tokenizer.word_index) + 1 #since indexing of array are zero-offset, the index of the vocabulary must be one larger than the length\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences) #transforming the sequens of integers to arrays\n",
    "X, y = sequences[:,:-1], sequences[:,-1] #defining X (input sequences) and y (output words)\n",
    "y = to_categorical(y, num_classes=vocab_size) #to_categorical converts a class vector (integers) to binary class matrix\n",
    "seq_length = X.shape[1] #gives you the dimension of the array, which we put to be 50\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the model with different layers. We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            123000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2460)              248460    \n",
      "=================================================================\n",
      "Total params: 522,360\n",
      "Trainable params: 522,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential() #assigning the sequential function to a model\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length)) #defining embedding layer size\n",
    "model.add(LSTM(100, return_sequences=True)) #adding layer of nodes\n",
    "model.add(LSTM(100))  #adding layer of nodes\n",
    "model.add(Dense(100, activation='relu')) #specifying the structure of the hidden layer, recu is an argument of a rectified linear unit. \n",
    "model.add(Dense(vocab_size, activation='softmax')) #using the softmax function to creating probabilities\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model, we train it on the training sequences and save it for the next lesson alongside with the tokenizer, which contains the translation of the tokens into integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "15523/15523 [==============================] - 44s 3ms/step - loss: 6.2251 - acc: 0.0759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e8ff77b630>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #the compile function configures the model for training specifying the categorical cross entropy loss\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=1) #training the model, orginally 100 epoch, but changed for illustration of output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson we created a model along side with the tokenizer dictionary, which we will start of by loading into the script. We will also load the cleaned sequences, which will be used to select a input sequence to prompt the model to produce new output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "doc = load_doc('Data/fairytales_sequences.txt')\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Specifying input sequences length to prompt the model\n",
    "seq_length = len(lines[3].split()) - 1 #splitting the sequences into words, counting them -1\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "model= load_model('model.h5')\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb')) #r for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behind them the skin of their feet the quills of their wings and the soft feathers of their breasts and when the frost had gone the salt water was torture for their wounds yet ever they sang their songs piercing sweet and speaking of the peace and joy to come and\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting a seed text\n",
    "seed_text = lines[randint(0,len(lines))] #returns random integer between 0 and how many lines there is, and indexes this. \n",
    "print(seed_text + '\\n') #prints the selected text\n",
    "len(seed_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting a seed text to prompt the model, we will translate it into integers that the model will know how to manipulate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating the input text to integers\n",
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can predict the next word directly by calling model.predict_classes() that will return the index of the word with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 353ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict probabilities for each word\n",
    "yhat = model.predict_classes(encoded, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking up the index in the Tokenizers mapping to get the associated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'measured'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making a loop to translate integer to word\n",
    "out_word = ''\n",
    "for word, index in tokenizer.word_index.items():\n",
    "\tif index == yhat:\n",
    "\t\tout_word = word\n",
    "\t\tbreak\n",
    "\n",
    "out_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sequences will get too long, in order to keep them to 50 items using the following function, which pads sequences to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the max length to be 50 items by removing items from the beginnning of the sequence\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the model to append the predicted word to the seed text and predict the following word by the new seed text, we create a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function, which generates the predicted output\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list() #make an empty list\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate an output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measured measured strings heed shrank hear hear limitless ledge penetrate penetrate penetrate bred limitless limitless limitless jammed jammed jammed redoubted redoubted sins swam redoubted swam redoubted redoubted damp redoubted redoubted dropping dropping treachery redoubted redoubted redoubted redoubted redoubted redoubted loch loch moaning moaning unexpected unexpected unexpected unexpected whelps unexpected unexpected unexpected whelps unexpected unexpected unexpected piece piece matter matter burst burst unalterable unalterable unalterable shields shields shields shields meet meet majesty majesty meet natural natural natural natural march march evade evade march fingernails march march march learned march tailleken concerning concerning concerning concerning priestly priestly hair concerning concerning concerning fly\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 100)\n",
    "print(generated)\n",
    "\n",
    "#saving generated text\n",
    "out_filename = 'Data/first_output.txt'\n",
    "save_doc(generated, out_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model appears to get stuck in one word. This is probably due to the fact, that the model has predicted the same words a couple of times in a row, which it has appended to the seed text. This is not strange considering the input sequences only change by one word. The more times it predicts the same word from the context the more likely it is it will predict it again. \n",
    "\n",
    "An example: Predicting 'measured' from the input sequence 'behind them the skin of their feet the quills of their wings and the soft feathers of their breasts and when the frost had gone the salt water was torture for their wounds yet ever they sang their songs piercing sweet and speaking of the peace and joy to come and' will become 'them the skin of their feet the quills of their wings and the soft feathers of their breasts and when the frost had gone the salt water was torture for their wounds yet ever they sang their songs piercing sweet and speaking of the peace and joy to come and measuring', which will lead the model to predict 'measuring' again, since the input did not differ that much. This will give the new input sequence: 'the skin of their feet the quills of their wings and the soft feathers of their breasts and when the frost had gone the salt water was torture for their wounds yet ever they sang their songs piercing sweet and speaking of the peace and joy to come and measuring measuring'. The more times measuring occurs in the end, the more likely it is it will predict 'measuring' again. \n",
    "\n",
    "A way to overcome this is maybe to give the model more data, make the input sequences differ more or give the model some elementary syntactic rules to progress from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Advancing the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson we were able to produce an output, though it might not have been ideal. To advance from here I would like to incorporate pre-trained Word Embeddings. Thus, I will extend the model to use pre-trained word2vec or GloVe vectors to see if it results in a better model. This might give the model a elementary idea of the basic syntax. \n",
    "\n",
    "First, I will need to find some pre-trained word vectors and download them, which is what i do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B/glove.6B.100d.txt', encoding = 'utf8')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will create a matrix containing the pre-trained word-vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix of weights are incorporated as a layer in the model, which I train to see, if I get a different result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 100)           246000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 100)           80400     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2460)              248460    \n",
      "=================================================================\n",
      "Total params: 665,360\n",
      "Trainable params: 419,360\n",
      "Non-trainable params: 246,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential() #assigning the sequential function to a model\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=seq_length, trainable = False)) #defining embedding layer size\n",
    "model.add(LSTM(100, return_sequences=True)) #adding layer of nodes\n",
    "model.add(LSTM(100))  #adding layer of nodes\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu')) #specifying the structure of the hidden layer, recu is an argument of a rectified linear unit. \n",
    "model.add(Dense(vocab_size, activation='softmax')) #using the softmax function to creating probabilities\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "15523/15523 [==============================] - 42s 3ms/step - loss: 6.5172 - acc: 0.0632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e8f7f92320>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #the compile function configures the model for training specifying the categorical cross entropy loss\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=1) #training the model, originally 100 epoch, but changed for illustration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next a save the model and tokenizer to prevent I will need to run the training session again. Select a seed text for the model and produce a generated output. This I save. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model_advanced.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "model= load_model('model_advanced.h5')\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb')) #r for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low ugly place but they do so no longer i dwell by the seaside says korsÃ¶r i have broad highroads and pleasure gardens and i have given birth to a poet a witty one too which is more than all poets are i once thought of sending a ship all round\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting a seed text\n",
    "seed_text = lines[randint(0,len(lines))] #returns random integer between 0 and how many lines there is, and indexes this. \n",
    "print(seed_text + '\\n') #prints the selected text\n",
    "len(seed_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 100)\n",
    "print(generated)\n",
    "\n",
    "#saving generated text\n",
    "out_filename = 'Data/first_output.txt'\n",
    "save_doc(generated, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is not from the full model, since I changed the number of iterations of the training section to allow output being shown without running the training session again. For the advanced output see appendix D. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ldm)",
   "language": "python",
   "name": "ldm_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
